/**
 * @file features/speechRecognition/hooks/useHybridSpeechRecognition.ts
 * @description í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ Hook
 *
 * Android 13+/iOS: expo-speech-recognition (ì‹¤ì‹œê°„ STT + WAV ë…¹ìŒ)
 * Android 12-: react-native-audio-record (WAV ë…¹ìŒë§Œ)
 *
 * ë‘ ë°©ì‹ ëª¨ë‘ 16kHz WAV íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ONNX ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 */

import { useState, useCallback, useEffect, useRef } from 'react';
import { Platform } from 'react-native';
import {
  ExpoSpeechRecognitionModule,
  useSpeechRecognitionEvent,
} from 'expo-speech-recognition';
import AudioRecord from 'react-native-audio-record';
import { Paths } from 'expo-file-system';

import { useSpeechRecognition as useSpeechRecognitionContext } from '../speechRecognitionContext';
import { KOREAN_LOCALE } from '../utils/koreanModelManager';

/**
 * ë…¹ìŒ ìƒíƒœ
 */
export type RecognitionStatus =
  | 'idle'           // ëŒ€ê¸° ì¤‘
  | 'starting'       // ì‹œì‘ ì¤‘
  | 'recognizing'    // ì¸ì‹ ì¤‘
  | 'stopping'       // ì¢…ë£Œ ì¤‘
  | 'completed'      // ì™„ë£Œ
  | 'error';         // ì˜¤ë¥˜

/**
 * í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ ê²°ê³¼
 */
export interface HybridRecognitionResult {
  /** ì‹¤ì‹œê°„ STT ê²°ê³¼ (Google/Siri ìì—°ì–´ ì²˜ë¦¬) - Android 13+/iOSë§Œ */
  realtimeTranscript: string | null;
  /** WAV íŒŒì¼ ê²½ë¡œ (ONNX ì²˜ë¦¬ìš©) */
  audioUri: string | null;
  /** ë…¹ìŒ ì‹œê°„ (ì´ˆ) */
  duration: number;
}

/**
 * Hook ë°˜í™˜ íƒ€ì…
 */
interface UseHybridSpeechRecognitionReturn {
  /** í˜„ì¬ ìƒíƒœ */
  status: RecognitionStatus;
  /** ì‹¤ì‹œê°„ STT ê²°ê³¼ (ë§í•˜ëŠ” ë™ì•ˆ ì—…ë°ì´íŠ¸) */
  realtimeTranscript: string;
  /** ìµœì¢… STT ê²°ê³¼ */
  finalTranscript: string;
  /** WAV íŒŒì¼ URI */
  audioUri: string | null;
  /** ë…¹ìŒ ì‹œê°„ (ì´ˆ) */
  duration: number;
  /** ì—ëŸ¬ ë©”ì‹œì§€ */
  error: string | null;
  /** í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ */
  canUseHybridMode: boolean;
  /** ë…¹ìŒ ì‹œì‘ */
  startRecognition: () => Promise<void>;
  /** ë…¹ìŒ ì¤‘ì§€ */
  stopRecognition: () => Promise<HybridRecognitionResult>;
  /** ìƒíƒœ ì´ˆê¸°í™” */
  reset: () => void;
}

/**
 * í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ Hook
 *
 * @description
 * Android 13+/iOSì—ì„œëŠ” expo-speech-recognitionì„ ì‚¬ìš©í•˜ì—¬:
 * - ì‹¤ì‹œê°„ STT (Google/Siri)
 * - WAV íŒŒì¼ ë…¹ìŒ (16kHz)
 *
 * Android 12 ì´í•˜ì—ì„œëŠ” react-native-audio-recordë¥¼ ì‚¬ìš©í•˜ì—¬:
 * - WAV íŒŒì¼ ë…¹ìŒë§Œ (16kHz)
 * - ì‹¤ì‹œê°„ STT ì—†ìŒ
 */
export function useHybridSpeechRecognition(): UseHybridSpeechRecognitionReturn {
  const { canUseHybridMode, capabilities } = useSpeechRecognitionContext();

  // ìƒíƒœ
  const [status, setStatus] = useState<RecognitionStatus>('idle');
  const [realtimeTranscript, setRealtimeTranscript] = useState('');
  const [finalTranscript, setFinalTranscript] = useState('');
  const [audioUri, setAudioUri] = useState<string | null>(null);
  const [duration, setDuration] = useState(0);
  const [error, setError] = useState<string | null>(null);

  // íƒ€ì´ë¨¸ ref
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const startTimeRef = useRef<number>(0);

  // í´ë°± ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (Android 12 ì´í•˜)
  const useFallbackMode = Platform.OS === 'android' && !canUseHybridMode;

  /**
   * expo-speech-recognition ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ
   */
  useSpeechRecognitionEvent('start', () => {
    console.log('[HybridSTT] ğŸ¤ ì¸ì‹ ì‹œì‘');
    setStatus('recognizing');
  });

  useSpeechRecognitionEvent('result', (event) => {
    const transcript = event.results[0]?.transcript || '';
    console.log('[HybridSTT] ğŸ“ ê²°ê³¼:', transcript, 'isFinal:', event.isFinal);

    if (event.isFinal) {
      setFinalTranscript(transcript);
    } else {
      setRealtimeTranscript(transcript);
    }
  });

  useSpeechRecognitionEvent('audioend', (event) => {
    console.log('[HybridSTT] ğŸ”Š ì˜¤ë””ì˜¤ ì¢…ë£Œ, URI:', event.uri);
    if (event.uri) {
      setAudioUri(event.uri);
    }
  });

  useSpeechRecognitionEvent('end', () => {
    console.log('[HybridSTT] âœ… ì¸ì‹ ì¢…ë£Œ');
    stopTimer();
    setStatus('completed');
  });

  useSpeechRecognitionEvent('error', (event) => {
    console.error('[HybridSTT] âŒ ì—ëŸ¬:', event.error, event.message);
    // 'aborted' ì—ëŸ¬ëŠ” ì •ìƒ ì¢…ë£Œë¡œ ì²˜ë¦¬
    if (event.error !== 'aborted') {
      setError(event.message || event.error);
      setStatus('error');
    }
    stopTimer();
  });

  /**
   * íƒ€ì´ë¨¸ ì‹œì‘
   */
  const startTimer = useCallback(() => {
    startTimeRef.current = Date.now();
    setDuration(0);

    timerRef.current = setInterval(() => {
      const elapsed = (Date.now() - startTimeRef.current) / 1000;
      setDuration(elapsed);
    }, 100);
  }, []);

  /**
   * íƒ€ì´ë¨¸ ì¤‘ì§€
   */
  const stopTimer = useCallback(() => {
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }
    const finalDuration = (Date.now() - startTimeRef.current) / 1000;
    setDuration(finalDuration);
    return finalDuration;
  }, []);

  /**
   * í´ë°± ëª¨ë“œ ë…¹ìŒ ì´ˆê¸°í™” (Android 12 ì´í•˜)
   */
  const initFallbackRecording = useCallback(() => {
    const fileName = `recording_${Date.now()}.wav`;
    AudioRecord.init({
      sampleRate: 16000,
      channels: 1,
      bitsPerSample: 16,
      audioSource: 6, // VOICE_RECOGNITION
      wavFile: fileName,
    });
    console.log('[HybridSTT] ğŸ“¼ í´ë°± ë…¹ìŒ ì´ˆê¸°í™”:', fileName);
  }, []);

  /**
   * ë…¹ìŒ ì‹œì‘
   */
  const startRecognition = useCallback(async () => {
    console.log('[HybridSTT] ğŸš€ ë…¹ìŒ ì‹œì‘ ìš”ì²­...');
    console.log('[HybridSTT] í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ:', canUseHybridMode);
    console.log('[HybridSTT] í´ë°± ëª¨ë“œ:', useFallbackMode);

    // ìƒíƒœ ì´ˆê¸°í™”
    setStatus('starting');
    setRealtimeTranscript('');
    setFinalTranscript('');
    setAudioUri(null);
    setError(null);
    setDuration(0);

    try {
      if (useFallbackMode) {
        // Android 12 ì´í•˜: react-native-audio-record ì‚¬ìš©
        console.log('[HybridSTT] ğŸ“¼ í´ë°± ëª¨ë“œë¡œ ë…¹ìŒ ì‹œì‘');
        initFallbackRecording();
        AudioRecord.start();
        setStatus('recognizing');
        startTimer();
      } else {
        // Android 13+/iOS: expo-speech-recognition ì‚¬ìš©
        console.log('[HybridSTT] ğŸ¤ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œë¡œ ë…¹ìŒ ì‹œì‘');

        // ê¶Œí•œ ìš”ì²­
        const permissionResult =
          await ExpoSpeechRecognitionModule.requestPermissionsAsync();
        if (!permissionResult.granted) {
          throw new Error('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
        }

        // ë…¹ìŒ ì‹œì‘ (ì‹¤ì‹œê°„ STT + WAV ì €ì¥)
        ExpoSpeechRecognitionModule.start({
          lang: KOREAN_LOCALE,
          interimResults: true,
          continuous: true,
          requiresOnDeviceRecognition: true, // ë¡œì»¬ ì „ìš©
          addsPunctuation: true,
          recordingOptions: {
            persist: true,
            outputDirectory: Paths.cache.uri,
            outputFileName: `recording_${Date.now()}.wav`,
            outputSampleRate: 16000, // ONNX ëª¨ë¸ìš© 16kHz
            outputEncoding: 'pcmFormatInt16', // 16-bit PCM
          },
        });

        startTimer();
      }
    } catch (err) {
      const errorMessage =
        err instanceof Error ? err.message : 'ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨';
      console.error('[HybridSTT] âŒ ì‹œì‘ ì‹¤íŒ¨:', errorMessage);
      setError(errorMessage);
      setStatus('error');
    }
  }, [canUseHybridMode, useFallbackMode, initFallbackRecording, startTimer]);

  /**
   * ë…¹ìŒ ì¤‘ì§€
   */
  const stopRecognition = useCallback(async (): Promise<HybridRecognitionResult> => {
    console.log('[HybridSTT] ğŸ›‘ ë…¹ìŒ ì¤‘ì§€ ìš”ì²­...');
    setStatus('stopping');

    const finalDuration = stopTimer();

    try {
      if (useFallbackMode) {
        // Android 12 ì´í•˜: react-native-audio-record ì¤‘ì§€
        const audioFile = await AudioRecord.stop();
        let fileUri = audioFile;
        if (Platform.OS === 'android' && !audioFile.startsWith('file://')) {
          fileUri = `file://${audioFile}`;
        }

        console.log('[HybridSTT] ğŸ“¼ í´ë°± ë…¹ìŒ ì™„ë£Œ:', fileUri);
        setAudioUri(fileUri);
        setStatus('completed');

        return {
          realtimeTranscript: null, // í´ë°± ëª¨ë“œì—ì„œëŠ” ì‹¤ì‹œê°„ STT ì—†ìŒ
          audioUri: fileUri,
          duration: finalDuration,
        };
      } else {
        // Android 13+/iOS: expo-speech-recognition ì¤‘ì§€
        ExpoSpeechRecognitionModule.stop();

        // 'end' ì´ë²¤íŠ¸ì—ì„œ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë¨
        // audioUriëŠ” 'audioend' ì´ë²¤íŠ¸ì—ì„œ ì„¤ì •ë¨

        // ê²°ê³¼ ëŒ€ê¸° (ì´ë²¤íŠ¸ê°€ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ë¨)
        await new Promise((resolve) => setTimeout(resolve, 500));

        return {
          realtimeTranscript: finalTranscript || realtimeTranscript,
          audioUri,
          duration: finalDuration,
        };
      }
    } catch (err) {
      const errorMessage =
        err instanceof Error ? err.message : 'ë…¹ìŒ ì¤‘ì§€ ì‹¤íŒ¨';
      console.error('[HybridSTT] âŒ ì¤‘ì§€ ì‹¤íŒ¨:', errorMessage);
      setError(errorMessage);
      setStatus('error');

      return {
        realtimeTranscript: null,
        audioUri: null,
        duration: finalDuration,
      };
    }
  }, [
    useFallbackMode,
    stopTimer,
    finalTranscript,
    realtimeTranscript,
    audioUri,
  ]);

  /**
   * ìƒíƒœ ì´ˆê¸°í™”
   */
  const reset = useCallback(() => {
    console.log('[HybridSTT] ğŸ”„ ìƒíƒœ ì´ˆê¸°í™”');
    setStatus('idle');
    setRealtimeTranscript('');
    setFinalTranscript('');
    setAudioUri(null);
    setDuration(0);
    setError(null);
    stopTimer();
  }, [stopTimer]);

  // ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    };
  }, []);

  return {
    status,
    realtimeTranscript,
    finalTranscript,
    audioUri,
    duration,
    error,
    canUseHybridMode,
    startRecognition,
    stopRecognition,
    reset,
  };
}
