/**
 * @file features/speechRecognition/hooks/useHybridSpeechRecognition.ts
 * @description í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ Hook
 *
 * Android 13+/iOS: expo-speech-recognition (ì‹¤ì‹œê°„ STT + WAV ë…¹ìŒ)
 * Android 12-: react-native-audio-record (WAV ë…¹ìŒë§Œ)
 *
 * ë‘ ë°©ì‹ ëª¨ë‘ 16kHz WAV íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ONNX ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 */

import { Paths } from "expo-file-system";
import {
  ExpoSpeechRecognitionModule,
  useSpeechRecognitionEvent,
} from "expo-speech-recognition";
import { useCallback, useEffect, useRef, useState } from "react";
import { Platform } from "react-native";
import AudioRecord from "react-native-audio-record";

import { useSpeechRecognition as useSpeechRecognitionContext } from "../speechRecognitionContext";
import { KOREAN_LOCALE } from "../utils/koreanModelManager";

// ë¬´ì‹œí•´ë„ ë˜ëŠ” ì—ëŸ¬ íƒ€ì… (ë…¹ìŒì€ ì •ìƒ ì™„ë£Œë¨)
const IGNORABLE_ERRORS = ["client", "aborted"];

/**
 * ë…¹ìŒ ìƒíƒœ
 */
export type RecognitionStatus =
  | "idle" // ëŒ€ê¸° ì¤‘
  | "starting" // ì‹œì‘ ì¤‘
  | "recognizing" // ì¸ì‹ ì¤‘
  | "stopping" // ì¢…ë£Œ ì¤‘
  | "completed" // ì™„ë£Œ
  | "error"; // ì˜¤ë¥˜

/**
 * í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ ê²°ê³¼
 */
export interface HybridRecognitionResult {
  /** ì‹¤ì‹œê°„ STT ê²°ê³¼ (Google/Siri ìì—°ì–´ ì²˜ë¦¬) - Android 13+/iOSë§Œ */
  realtimeTranscript: string | null;
  /** WAV íŒŒì¼ ê²½ë¡œ (ONNX ì²˜ë¦¬ìš©) */
  audioUri: string | null;
  /** ë…¹ìŒ ì‹œê°„ (ì´ˆ) */
  duration: number;
}

/**
 * Hook ë°˜í™˜ íƒ€ì…
 */
interface UseHybridSpeechRecognitionReturn {
  /** í˜„ì¬ ìƒíƒœ */
  status: RecognitionStatus;
  /** ì‹¤ì‹œê°„ STT ê²°ê³¼ (ë§í•˜ëŠ” ë™ì•ˆ ì—…ë°ì´íŠ¸) */
  realtimeTranscript: string;
  /** ìµœì¢… STT ê²°ê³¼ */
  finalTranscript: string;
  /** WAV íŒŒì¼ URI */
  audioUri: string | null;
  /** ë…¹ìŒ ì‹œê°„ (ì´ˆ) */
  duration: number;
  /** ì—ëŸ¬ ë©”ì‹œì§€ */
  error: string | null;
  /** í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ */
  canUseHybridMode: boolean;
  /** ë…¹ìŒ ì‹œì‘ */
  startRecognition: () => Promise<void>;
  /** ë…¹ìŒ ì¤‘ì§€ */
  stopRecognition: () => Promise<HybridRecognitionResult>;
  /** ìƒíƒœ ì´ˆê¸°í™” */
  reset: () => void;
}

/**
 * í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ Hook
 *
 * @description
 * Android 13+/iOSì—ì„œëŠ” expo-speech-recognitionì„ ì‚¬ìš©í•˜ì—¬:
 * - ì‹¤ì‹œê°„ STT (Google/Siri)
 * - WAV íŒŒì¼ ë…¹ìŒ (16kHz)
 *
 * Android 12 ì´í•˜ì—ì„œëŠ” react-native-audio-recordë¥¼ ì‚¬ìš©í•˜ì—¬:
 * - WAV íŒŒì¼ ë…¹ìŒë§Œ (16kHz)
 * - ì‹¤ì‹œê°„ STT ì—†ìŒ
 */
export function useHybridSpeechRecognition(): UseHybridSpeechRecognitionReturn {
  const { canUseHybridMode, capabilities } = useSpeechRecognitionContext();

  // ìƒíƒœ
  const [status, setStatus] = useState<RecognitionStatus>("idle");
  const [realtimeTranscript, setRealtimeTranscript] = useState("");
  const [finalTranscript, setFinalTranscript] = useState("");
  const [audioUri, setAudioUri] = useState<string | null>(null);
  const [duration, setDuration] = useState(0);
  const [error, setError] = useState<string | null>(null);

  // íƒ€ì´ë¨¸ ref
  const timerRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const startTimeRef = useRef<number>(0);

  // ë¹„ë™ê¸° ì´ë²¤íŠ¸ì—ì„œ ê°’ì„ ë™ê¸°ì ìœ¼ë¡œ ì¶”ì í•˜ê¸° ìœ„í•œ refl;l;
  const audioUriRef = useRef<string | null>(null);
  const transcriptRef = useRef<string>("");

  // audioend ì´ë²¤íŠ¸ ëŒ€ê¸°ë¥¼ ìœ„í•œ Promise resolver
  const audioEndResolverRef = useRef<((uri: string | null) => void) | null>(
    null
  );

  // í´ë°± ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (Android 12 ì´í•˜)
  //TODO í´ë°± í…ŒìŠ¤íŠ¸
  const useFallbackMode = Platform.OS === "android" && !canUseHybridMode;

  /**
   * expo-speech-recognition ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ
   */
  useSpeechRecognitionEvent("start", () => {
    console.log("[HybridSTT] ğŸ¤ ì¸ì‹ ì‹œì‘");
    setStatus("recognizing");
  });

  useSpeechRecognitionEvent("result", (event) => {
    const transcript = event.results[0]?.transcript || "";
    const now = Date.now();
    const elapsed =
      startTimeRef.current > 0
        ? ((now - startTimeRef.current) / 1000).toFixed(2)
        : "0.00";

    console.log(
      `[HybridSTT] ğŸ“ ê²°ê³¼ (${elapsed}s):`,
      transcript,
      "isFinal:",
      event.isFinal
    );

    if (event.isFinal) {
      console.log(`[HybridSTT] ğŸ¯ isFinal=true ìˆ˜ì‹ ! (${elapsed}s)`);
    }

    // refì—ë„ ì €ì¥ (stopRecognitionì—ì„œ ë™ê¸°ì ìœ¼ë¡œ ì ‘ê·¼í•˜ê¸° ìœ„í•´)
    transcriptRef.current = transcript;

    if (event.isFinal) {
      setFinalTranscript(transcript);
    } else {
      setRealtimeTranscript(transcript);
    }
  });

  useSpeechRecognitionEvent("audioend", (event) => {
    console.log("[HybridSTT] ğŸ”Š ì˜¤ë””ì˜¤ ì¢…ë£Œ, URI:", event.uri);
    const uri = event.uri || null;

    // refì™€ state ëª¨ë‘ ì—…ë°ì´íŠ¸
    audioUriRef.current = uri;
    setAudioUri(uri);

    // ëŒ€ê¸° ì¤‘ì¸ Promiseê°€ ìˆìœ¼ë©´ resolve
    if (audioEndResolverRef.current) {
      audioEndResolverRef.current(uri);
      audioEndResolverRef.current = null;
    }
  });

  useSpeechRecognitionEvent("end", () => {
    console.log("[HybridSTT] âœ… ì¸ì‹ ì¢…ë£Œ");
    stopTimer();
    setStatus("completed");
  });

  useSpeechRecognitionEvent("error", (event) => {
    const now = Date.now();
    const elapsed =
      startTimeRef.current > 0
        ? ((now - startTimeRef.current) / 1000).toFixed(2)
        : "0.00";

    // ë¬´ì‹œí•´ë„ ë˜ëŠ” ì—ëŸ¬ëŠ” ì •ìƒ ì¢…ë£Œë¡œ ì²˜ë¦¬ (ë…¹ìŒ íŒŒì¼ì€ ì •ìƒ ìƒì„±ë¨)
    if (IGNORABLE_ERRORS.includes(event.error)) {
      console.log(
        `[HybridSTT] âš ï¸ ë¬´ì‹œ ê°€ëŠ¥í•œ ì—ëŸ¬ (${elapsed}s):`,
        event.error,
        event.message
      );
    } else {
      console.error(
        `[HybridSTT] âŒ ì—ëŸ¬ (${elapsed}s):`,
        event.error,
        event.message
      );
      setError(event.message || event.error);
      setStatus("error");
    }
    stopTimer();
  });

  /**
   * íƒ€ì´ë¨¸ ì‹œì‘
   */
  const startTimer = useCallback(() => {
    startTimeRef.current = Date.now();
    setDuration(0);

    timerRef.current = setInterval(() => {
      const elapsed = (Date.now() - startTimeRef.current) / 1000;
      setDuration(elapsed);
    }, 100);
  }, []);

  /**
   * íƒ€ì´ë¨¸ ì¤‘ì§€
   */
  const stopTimer = useCallback(() => {
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }
    const finalDuration = (Date.now() - startTimeRef.current) / 1000;
    setDuration(finalDuration);
    return finalDuration;
  }, []);

  /**
   * í´ë°± ëª¨ë“œ ë…¹ìŒ ì´ˆê¸°í™” (Android 12 ì´í•˜)
   */
  const initFallbackRecording = useCallback(() => {
    const fileName = `recording_${Date.now()}.wav`;
    AudioRecord.init({
      sampleRate: 16000,
      channels: 1,
      bitsPerSample: 16,
      audioSource: 6, // VOICE_RECOGNITION
      wavFile: fileName,
    });
    console.log("[HybridSTT] ğŸ“¼ í´ë°± ë…¹ìŒ ì´ˆê¸°í™”:", fileName);
  }, []);

  /**
   * ë…¹ìŒ ì‹œì‘
   */
  const startRecognition = useCallback(async () => {
    console.log("[HybridSTT] ğŸš€ ë…¹ìŒ ì‹œì‘ ìš”ì²­...");
    console.log("[HybridSTT] í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ:", canUseHybridMode);
    console.log("[HybridSTT] í´ë°± ëª¨ë“œ:", useFallbackMode);

    // ìƒíƒœ ì´ˆê¸°í™”
    setStatus("starting");
    setRealtimeTranscript("");
    setFinalTranscript("");
    setAudioUri(null);
    setError(null);
    setDuration(0);

    // ref ì´ˆê¸°í™”
    audioUriRef.current = null;
    transcriptRef.current = "";
    audioEndResolverRef.current = null;

    try {
      if (useFallbackMode) {
        // Android 12 ì´í•˜: react-native-audio-record ì‚¬ìš©
        console.log("[HybridSTT] ğŸ“¼ í´ë°± ëª¨ë“œë¡œ ë…¹ìŒ ì‹œì‘ - ìì—°ì–´STT X ");
        initFallbackRecording();
        AudioRecord.start();
        setStatus("recognizing");
        startTimer();
      } else {
        // Android 13+/iOS: expo-speech-recognition ì‚¬ìš©
        console.log("[HybridSTT] ğŸ¤ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œë¡œ ë…¹ìŒ ì‹œì‘ - ìì—°ì–´STT O");

        // ê¶Œí•œ ìš”ì²­
        const permissionResult =
          await ExpoSpeechRecognitionModule.requestPermissionsAsync();
        if (!permissionResult.granted) {
          throw new Error("ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.");
        }

        // ë…¹ìŒ ì‹œì‘ (ì‹¤ì‹œê°„ STT + WAV ì €ì¥)
        ExpoSpeechRecognitionModule.start({
          lang: KOREAN_LOCALE,
          interimResults: true,
          continuous: true,
          requiresOnDeviceRecognition: true, // ë¡œì»¬ ì „ìš©
          addsPunctuation: true,
          recordingOptions: {
            persist: true,
            outputDirectory: Paths.cache.uri,
            outputFileName: `recording_${Date.now()}.wav`,
            outputSampleRate: 16000, // ONNX ëª¨ë¸ìš© 16kHz
            outputEncoding: "pcmFormatInt16", // 16-bit PCM
          },
        });

        startTimer();
      }
    } catch (err) {
      const errorMessage =
        err instanceof Error ? err.message : "ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨";
      console.error("[HybridSTT] âŒ ì‹œì‘ ì‹¤íŒ¨:", errorMessage);
      setError(errorMessage);
      setStatus("error");
    }
  }, [canUseHybridMode, useFallbackMode, initFallbackRecording, startTimer]);

  /**
   * ë…¹ìŒ ì¤‘ì§€
   */
  const stopRecognition =
    useCallback(async (): Promise<HybridRecognitionResult> => {
      const now = Date.now();
      const elapsed =
        startTimeRef.current > 0
          ? ((now - startTimeRef.current) / 1000).toFixed(2)
          : "0.00";
      console.log(`[HybridSTT] ğŸ›‘ ë…¹ìŒ ì¤‘ì§€ ìš”ì²­ (${elapsed}s)...`);
      setStatus("stopping");

      const finalDuration = stopTimer();

      try {
        if (useFallbackMode) {
          // Android 12 ì´í•˜: react-native-audio-record ì¤‘ì§€
          const audioFile = await AudioRecord.stop();
          let fileUri = audioFile;
          if (Platform.OS === "android" && !audioFile.startsWith("file://")) {
            fileUri = `file://${audioFile}`;
          }

          console.log("[HybridSTT] ğŸ“¼ í´ë°± ë…¹ìŒ ì™„ë£Œ:", fileUri);
          setAudioUri(fileUri);
          setStatus("completed");

          return {
            realtimeTranscript: null, // í´ë°± ëª¨ë“œì—ì„œëŠ” ì‹¤ì‹œê°„ STT ì—†ìŒ
            audioUri: fileUri,
            duration: finalDuration,
          };
        } else {
          // Android 13+/iOS: expo-speech-recognition ì¤‘ì§€
          ExpoSpeechRecognitionModule.stop();

          // 'audioend' ì´ë²¤íŠ¸ ëŒ€ê¸° (ìµœëŒ€ 3ì´ˆ)
          // ì´ë¯¸ audioendê°€ ë„ì°©í–ˆìœ¼ë©´ refì—ì„œ ë°”ë¡œ ê°€ì ¸ì˜´
          let resolvedAudioUri = audioUriRef.current;

          if (!resolvedAudioUri) {
            console.log("[HybridSTT] â³ audioend ì´ë²¤íŠ¸ ëŒ€ê¸° ì¤‘...");
            resolvedAudioUri = await Promise.race([
              new Promise<string | null>((resolve) => {
                audioEndResolverRef.current = resolve;
              }),
              new Promise<string | null>((resolve) =>
                setTimeout(() => resolve(null), 3000)
              ),
            ]);
          }

          console.log("[HybridSTT] âœ… ìµœì¢… audioUri:", resolvedAudioUri);
          console.log("[HybridSTT] âœ… ìµœì¢… transcript:", transcriptRef.current);

          return {
            realtimeTranscript:
              transcriptRef.current || finalTranscript || realtimeTranscript,
            audioUri: resolvedAudioUri,
            duration: finalDuration,
          };
        }
      } catch (err) {
        const errorMessage =
          err instanceof Error ? err.message : "ë…¹ìŒ ì¤‘ì§€ ì‹¤íŒ¨";
        console.error("[HybridSTT] âŒ ì¤‘ì§€ ì‹¤íŒ¨:", errorMessage);
        setError(errorMessage);
        setStatus("error");

        return {
          realtimeTranscript: null,
          audioUri: null,
          duration: finalDuration,
        };
      }
    }, [
      useFallbackMode,
      stopTimer,
      finalTranscript,
      realtimeTranscript,
      audioUri,
    ]);

  /**
   * ìƒíƒœ ì´ˆê¸°í™”
   */
  const reset = useCallback(() => {
    console.log("[HybridSTT] ğŸ”„ ìƒíƒœ ì´ˆê¸°í™”");
    setStatus("idle");
    setRealtimeTranscript("");
    setFinalTranscript("");
    setAudioUri(null);
    setDuration(0);
    setError(null);
    stopTimer();

    // ref ì´ˆê¸°í™”
    audioUriRef.current = null;
    transcriptRef.current = "";
    audioEndResolverRef.current = null;
  }, [stopTimer]);

  // ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    };
  }, []);

  return {
    status,
    realtimeTranscript,
    finalTranscript,
    audioUri,
    duration,
    error,
    canUseHybridMode,
    startRecognition,
    stopRecognition,
    reset,
  };
}
